#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
The idea of predicting source codes with data-driven approaches for source
 code prediction and autocompletion has been a topic of great interest.
 Statistical prediction models using n-grams
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "raychev14cc,nguyen13ngram,tu14ngram"

\end_inset

, decision trees
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "raychev16decision"

\end_inset

, graphs
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "nquyen15graph"

\end_inset

, domain specific languages
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "bielik16phog,raychev16lpn"

\end_inset

 have shown to predict source code components with decent accuracy.
 These models usually require a heavily processed version of the program
 that incorporates structural information given in forms such as abstract
 syntax trees
\begin_inset space ~
\end_inset

(AST) or context-free grammars
\begin_inset space \thinspace{}
\end_inset

(CFG).
\end_layout

\begin_layout Standard
Meanwhile, deep learning models have been known to successfully interpret
 the syntax information of text sequences without given any prior information,
 which has resulted in many studies that incorporate such models for source
 codes.
 
\begin_inset CommandInset citation
LatexCommand cite
key "dong16lang"

\end_inset

 use sequence-to-sequence neural networks to generate source codes from
 natural languages.
 
\begin_inset CommandInset citation
LatexCommand cite
key "white15soft"

\end_inset

 report that using RNNs instead of n-grams lowers the perplexity of a language
 model on source codes tokens.
 
\begin_inset CommandInset citation
LatexCommand cite
key "bhoopchand16spn"

\end_inset

 introduce a sequence-to-sequence code autocompletion model that predicts
 one token at a time by either generating a new token or using an identifier
 introduced earlier in that code.
 
\begin_inset CommandInset citation
LatexCommand cite
key "ling16lpn"

\end_inset

 propose the latent predictor network, a neural network for sequence generation
 which selects from several different models for generating tokens in different
 parts of a code.
 
\begin_inset CommandInset citation
LatexCommand cite
key "rabinovich17asn"

\end_inset

 further develop this idea by instead predicting the AST structure of a
 code.
\end_layout

\begin_layout Standard
Our model incorporates a recent trend in the concept of attention in neural
 network models for sequences.
 Following the work 
\begin_inset CommandInset citation
LatexCommand cite
key "vinyals15pointer"

\end_inset

 which extended the use of computed attention as a direct output, many studies
 on neural models that point to parts of the input sequence as answers have
 appeared.
 This approach has been widely adopted in natural language processing tasks,
 with models for generating sequences by copying relevant parts from input
 sequences seen in.sentence generation
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "gu16copynet"

\end_inset

, text summarization
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "cheng16sum,see17point"

\end_inset

, neural machine translation
\begin_inset space \thinspace{}
\end_inset

(NMT)
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "gulcehre16words"

\end_inset

 Similarly, our model also produces output sequences by both generating
 raw tokens and selectively copying them from previous lines of code.
\end_layout

\end_body
\end_document
