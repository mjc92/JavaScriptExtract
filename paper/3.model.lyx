#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Subsection
Task definition
\end_layout

\begin_layout Standard
We define our proposed task as can be seen at Fig.
\begin_inset space \thinspace{}
\end_inset


\shape italic

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:task"

\end_inset


\shape default
.
 Here we are given a source code snippet consisting of 
\begin_inset Formula $N+1$
\end_inset

 lines, where we denote the first 
\begin_inset Formula $N$
\end_inset

 lines as 
\series bold
source lines
\series default
 and the 
\begin_inset Formula $N+1$
\end_inset

th line as the 
\series bold
query line
\series default
.
 The query line is an incomplete line of source code which has to be completed
 by referring to previous lines of code and learning to generate the rest
 of the query line, which we denote as the 
\series bold
target line
\series default
.
 We hypothesize that there is at least one line of code from the source
 lines, namely the 
\series bold
reference line
\series default
, which contains structural information and relevant tokens that are required
 to reconstruct the target line.
 As the target line to be generated is in fact the remaining part of the
 incomplete query line, the reference line should be at least partially
 similar to the query line.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-0.8in}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/figure_task_overview/task.pdf
	width 150col%
	BoundingBox 0bp 0bp 960bp 500bp

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Task overview.
 The colors in `Selective copy' indicate which part of the input is it possible
 to extract a token for the target line
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:task"

\end_inset


\end_layout

\end_inset

In this sense we can divide our task into two steps: (1) to correctly select
 the reference line out of 
\begin_inset Formula $N$
\end_inset

 source lines by selecting the line that bears most resemblance to the query
 line, and (2) to use both the selected reference line and the query line
 to learn the line structure and tokens required for predicting the target
 line.
 This leads us to designing a two-staged neural network model which is suitable
 for such tasks.
\begin_inset Float figure
placement t
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-1.4in}
\end_layout

\end_inset


\begin_inset Graphics
	filename figures/figure_model_overview/model.pdf
	scale 50
	BoundingBox 0bp 0bp 960bp 500bp

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Model overview
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Model description
\end_layout

\begin_layout Standard
In this section, we will refer to the source lines as 
\begin_inset Formula $S$
\end_inset

 or 
\begin_inset Formula $\left[S_{1},S_{2},...,S_{n}\right]$
\end_inset

, the query line as 
\begin_inset Formula $Q$
\end_inset

, and the desired target line as 
\begin_inset Formula $Y$
\end_inset

.
 Concatenating 
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 results in a single line of source code.
 Note that each line is considered as a list of source code tokens, and
 all tokens in 
\begin_inset Formula $S$
\end_inset

,
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are represented as integers corresponding to the index of that token from
 a predefined vocabulary 
\begin_inset Formula $V$
\end_inset

.
 We use the notation 
\begin_inset Formula $len\left(X\right)$
\end_inset

 to indicate the length of a code line 
\begin_inset Formula $X$
\end_inset

.
 We represent the 
\begin_inset Formula $j$
\end_inset

th token of source line 
\begin_inset Formula $S_{i}$
\end_inset

 and the query as 
\begin_inset Formula $s_{i,j}$
\end_inset

 and 
\begin_inset Formula $q_{j}$
\end_inset

 respectively.
 
\end_layout

\begin_layout Subsubsection
Selector network
\end_layout

\begin_layout Standard
Our first task is to select the reference line 
\begin_inset Formula $S_{ref}$
\end_inset

 from 
\begin_inset Formula $S$
\end_inset

 by comparing the similarity of each line 
\begin_inset Formula $S_{i}$
\end_inset


\begin_inset Formula $\in S$
\end_inset

 with 
\begin_inset Formula $Q$
\end_inset

.
 As each line is a list of tokens, we can obtain the similarity score by
 comparing the tokens from each line.
 Although each of the lines are likely to have different lengths, we only
 have to compare up to 
\begin_inset Formula $len(Q)$
\end_inset

 tokens.
 Therefore, the task of measuring the similarity of the 
\begin_inset Formula $i$
\end_inset

th line of 
\begin_inset Formula $S$
\end_inset

 with 
\begin_inset Formula $Q$
\end_inset

 becomes analogous to comparing 
\begin_inset Formula $s_{i,j}$
\end_inset

 and 
\begin_inset Formula $q_{j}$
\end_inset

 where 
\begin_inset Formula $1\leq j\leq len(Q)$
\end_inset

 and adding up the similarity scores for each tokenwise pair.
\end_layout

\begin_layout Standard
How can we measure the similarity between tokens? While the most straightforward
 approach may be to directly compare the similarity of tokens as a binary
 problem and add them up as in
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{alignedat}{1}sim_{\text{tok}}\left(S_{i},Q\right)= & \frac{\text{\ensuremath{\left(\sum\limits _{j=1}^{len(\text{Q})}\ensuremath{\text{if}\thinspace}s_{i,j}\text{==\ensuremath{q_{j}}}\thinspace\text{then}\thinspace1\thinspace\ensuremath{\text{else}\,}\ensuremath{0}\right)}}}{len\left(Q\right)}\end{alignedat}
\label{eq:sim1}
\end{equation}

\end_inset

, we adopt a more sophisticated approach that incorporates information of
 both a token's meaning and surrounding semantics.
 We first create a word embedding matrix 
\begin_inset Formula $W_{emb}$
\end_inset

of size 
\begin_inset Formula $\mathbb{R}^{\left|V\right|\times d_{w}}$
\end_inset

, then embed each token of 
\begin_inset Formula $S_{i}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 to obtain 
\begin_inset Formula $d_{w}$
\end_inset

-dimensional word embedding vectors, which we denote as 
\begin_inset Formula $\mathbf{s}_{i,j}^{emb}$
\end_inset

 and 
\begin_inset Formula $\mathbf{q}_{j}^{emb}$
\end_inset

.
 We then apply a LSTM on these embeddings to obtain 
\begin_inset Formula $d_{h}$
\end_inset

-dimensional hidden states 
\begin_inset Formula $\mathbf{s}_{i,j}^{h}$
\end_inset

 and 
\begin_inset Formula $\mathbf{q}_{j}^{h}$
\end_inset

as follows:
\begin_inset Formula 
\begin{align}
\mathbf{s}_{i,j}^{h} & =\text{LSTM}\left(\mathbf{s}_{i,j-1}^{h},\mathbf{s}_{i,j}^{emb}\right)\label{eq:lstm}\\
\mathbf{q}_{j}^{h} & =\text{LSTM}\left(\mathbf{q}_{j-1}^{h},\mathbf{q}_{j}^{emb}\right)\label{eq:lstm2}
\end{align}

\end_inset

.
 Here, each hidden state becomes a vector representation of a token.
 We can then modify Eq.
\begin_inset space \thinspace{}
\end_inset

(1) as
\begin_inset Formula 
\begin{equation}
sim_{\text{LSTM}}\left(S_{i},Q\right)=\text{\ensuremath{\left(\sum\limits _{j=1}^{len(\text{Q})}cos\left(\mathbf{s}_{i,j}^{h},\mathbf{q}{}_{j}^{h}\right)\right)}}\label{eq:sim2}
\end{equation}

\end_inset

where we use vectors instead of raw tokens.To change the similarity scores
 to a differentiable probability distribution, we apply a softmax function
 on the 
\begin_inset Formula $n$
\end_inset

 similarity scores and derive a loss function as
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\begin_inset Formula 
\begin{align}
p(y & =i|q)=\text{softmax}\left(sim_{\text{LSTM}}\left((S_{i},Q\right)\right)\label{eq:soft}\\
\mathcal{L}_{1} & =-log\left[p\left(ref\thinspace|q\right)\right]\label{eq:loss1}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
.
 We can then choose the most similar line from 
\begin_inset Formula $S$
\end_inset

 by selecting the line with the highest probability.
\end_layout

\begin_layout Subsubsection
Copy-generator network
\end_layout

\begin_layout Standard
The inputs for the copy-generator network are 
\begin_inset Formula $S_{ref}$
\end_inset

 and the query line 
\begin_inset Formula $Q$
\end_inset

 which we concatenate as 
\begin_inset Formula $X$
\end_inset

.
 The objective of this model is to copy tokens selectively from 
\begin_inset Formula $X$
\end_inset

 to generate the corresponding target sequence 
\begin_inset Formula $Y$
\end_inset

.
 For this task, we construct an encoder-decoder model based on Copynet
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "gu16copynet"

\end_inset

.
\end_layout

\begin_layout Standard
Given a sequence 
\begin_inset Formula $X=\left\{ x_{1},...,x_{len(X)}\right\} $
\end_inset

, we obtain word embeddings 
\begin_inset Formula $\left\{ x_{1}^{emb},...,x_{len(X)}^{emb}\right\} $
\end_inset

 using the same word embedding matrix 
\begin_inset Formula $W_{emb}.$
\end_inset

 We feed this input into an encoder model which is a GRU to obtain 
\begin_inset Formula $d_{h}$
\end_inset

-dimensional hidden states 
\begin_inset Formula $\mathbf{h}{}_{1}^{e},...,\mathbf{h}{}_{len(X)}^{e}$
\end_inset

 for each input token, i.e.
 
\begin_inset Formula 
\begin{equation}
\mathbf{h}{}_{t}^{e}=GRU^{enc}\left(\mathbf{h}{}_{t-1}^{e},x_{t}\right),\thinspace t\in\left\{ 1,\ldots,len(X)\right\} ;\,\mathbf{h}{}_{0}^{e}=\mathbf{0}\label{eq:enc}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
with 
\series bold

\begin_inset Formula $\mathbf{h}{}_{0}^{e}$
\end_inset


\series default
 set to a zero vector.
 We then use the last hidden state 
\begin_inset Formula $\mathbf{h}{}_{len(X)}^{e}$
\end_inset

 as an encoded version of the input sequence, and feed it into the subsequent
 decoder model.
 At the decoder model, another GRU is used to generate hidden states as
\begin_inset Formula 
\begin{align}
\mathbf{h}{}_{t+1}^{d} & =GRU^{dec}\left(\mathbf{h}{}_{t}^{d},\left[\mathbf{c_{t}},y_{t}^{emb}\right]\right)\label{eq:dec}\\
\mathbf{h}{}_{0}^{d} & =\mathbf{h}{}_{len(X)}^{e};\thinspace c_{0}=\mathbf{0}
\end{align}

\end_inset

 where 
\begin_inset Formula $\mathbf{h}{}_{t}^{d}\in\mathbb{R}^{d_{h}}$
\end_inset

 is the 
\begin_inset Formula $t$
\end_inset

th hidden state for the decoder, 
\begin_inset Formula $\mathbf{c_{t}}\in\mathbb{R}^{d_{h}}$
\end_inset

 is the context vector initialized to a zero vector of , and 
\begin_inset Formula $y_{t}^{emb}\in\mathbb{R}^{d_{w}}$
\end_inset

 is the word embedding of the 
\begin_inset Formula $i$
\end_inset

th decoder input 
\begin_inset Formula $y_{t}\in Y$
\end_inset

, which is also embedded using 
\begin_inset Formula $W_{emb}$
\end_inset

.
 Each hidden state of the decoder 
\begin_inset Formula $\mathbf{h}{}_{t}^{d}$
\end_inset

 is used to predict the next word in two different ways: 
\begin_inset Formula $\left(1\right)$
\end_inset

 to obtain the attention scores of which part of the input sequence to attend,
 and 
\begin_inset Formula $\left(2\right)$
\end_inset

 to directly apply a linear transformation for generating the next word
 from a large vocabulary.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\left(1\right)$
\end_inset

, we calculate the output distribution of the predicted next word as 
\begin_inset Formula 
\begin{equation}
\mathbf{gen_{t}}=\left(\mathbf{h}{}_{t}^{d}\right)^{\mathrm{\mathsf{T}}}W_{o};\thinspace W_{o}\in\mathbb{R}^{d_{h}\times|V|};\label{eq:gen}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
.
 The output 
\begin_inset Formula $\mathbf{gen_{t}}$
\end_inset

 is a 
\begin_inset Formula $|V|$
\end_inset

-dimensional vector in which dimensions containing larger values indicate
 a higher probability of a word existing in vocabulary index equivalent
 to the dimension number.
\end_layout

\begin_layout Standard
For 
\begin_inset Formula $\left(2\right)$
\end_inset

, we use 
\begin_inset Formula $\mathbf{h}{}_{t}^{d}$
\end_inset

 to calculate attention scores for each of the hidden states from the input
 sequence as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\mathbf{att_{t,i}}=tanh\left(\left(\mathbf{h}{}_{t}^{d}\right)^{\mathsf{T}}e_{i}\right);\thinspace i\in\left\{ 1,\ldots,len(X)\right\} \label{eq:att}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
, which is the attention score for the 
\begin_inset Formula $i$
\end_inset

th token in the input sequence.
 For each decoder step 
\begin_inset Formula $t$
\end_inset

 we can calculate an attention vector 
\begin_inset Formula $\mathbf{att_{t}}\in\mathbb{\mathbb{R}}^{len(X)}$
\end_inset

 which represents how relevant each of the input tokens are to the word
 to be predicted at decoder step 
\begin_inset Formula $t$
\end_inset

.
 By concatenating 
\begin_inset Formula $\mathbf{att_{t}}$
\end_inset

 and 
\begin_inset Formula $\mathbf{gen_{t}}$
\end_inset

 and applying softmax, we can get a probability distribution vector and
 divide it into two parts as follows
\begin_inset Formula 
\begin{equation}
\mathbf{p}_{t}=\text{softmax}\left(\left[\mathbf{gen_{t}},\mathbf{att_{t,i}}\right]\right)\label{eq:p}
\end{equation}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.15in}
\end_layout

\end_inset


\begin_inset Formula 
\begin{equation}
\mathbf{p}_{t}^{gen}=\mathbf{p}_{t}\left[0:|V|\right]\label{eq:att-2}
\end{equation}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.15in}
\end_layout

\end_inset


\begin_inset Formula 
\begin{equation}
\mathbf{p}_{t}^{copy}=\mathbf{p}_{t}\left[|V|:|V|+len(X)\right]\label{eq:att-3}
\end{equation}

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
which contains both the probabilities of either copying the next token from
 the one of the input tokens or generating it directly using the hidden
 state.
 If the largest value is in the 
\begin_inset Formula $k$
\end_inset

th index of the vector 
\begin_inset Formula $\mathbf{p}_{t}^{copy}$
\end_inset

,
\begin_inset Formula $x_{k}$
\end_inset

 becomes the next output token.
 On the otherpackages, most tokens inveitably have to be treated as unknown
 words.
 In natural language processing tasks, these word are treated as out-of-vocabula
ry
\begin_inset space \thinspace{}
\end_inset

(OOV) tokens and are usually hand, if the largest value is located in the
 
\begin_inset Formula $k$
\end_inset

th index of 
\begin_inset Formula $\mathbf{p}_{t}^{gen}$
\end_inset

, the output token becomes the 
\begin_inset Formula $k$
\end_inset

th word in the predefined vocabulary 
\begin_inset Formula $V$
\end_inset

.
 This way, we can leverage between generating tokens from our vocabulary
 and copying them from the input sequence.
\end_layout

\begin_layout Standard
During each decoder step, we feed into the decoder model a context vector
 
\begin_inset Formula $\mathbf{c_{t}}$
\end_inset

 which is calculated as
\begin_inset Formula 
\begin{equation}
\mathbf{c_{t}}=\sum\limits _{j=1}^{len(X)}\ensuremath{\text{if}\,\,}x_{j}\text{==\ensuremath{y_{t}}}\,\,\text{then}\mathbf{\,\,att_{t,i}}\mathbf{h}{}_{i}^{e}\,\thinspace\text{else}\,\ensuremath{\,}\ensuremath{0}\label{eq:context}
\end{equation}

\end_inset

.
 This output represents how much information from the input sequence should
 the model actually need for each step.
 This model is different from other attention-based sequence-to-sequence
 models
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "bahdanau15att"

\end_inset

 in that instead of using the entire attention vector 
\begin_inset Formula $\mathbf{att_{t}}$
\end_inset

 to create a context vector, it only attends to positions where the input
 token is equal to the decoder input and omits all other positions.
 This rather direct form of attention allows for copying long sequences
 of tokens from the input as mentioned in 
\begin_inset CommandInset citation
LatexCommand cite
key "gu16copynet"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Dealing with out-of-vocabulary tokens
\end_layout

\begin_layout Standard
One advantage of our model is that it can handle words that does not exist
 in the predefined vocabulary.
 There is an unlimited amount of tokens that can appear in a source code
 text as different users may come up with different naming conventions for
 functions and variables.
 With the exception of symbols that constitute the grammar of a language
 or identifiers in frequently used packages, most tokens inveitably have
 to be treated as unknown words.
 In natural language processing tasks, these word are treated as out-of-vocabula
ry
\begin_inset space \thinspace{}
\end_inset

(OOV) tokens and are usually replaced with an 
\begin_inset Formula $\left\langle UNK\right\rangle $
\end_inset

 token.
 While we also use this convention while feeding unknown tokens into our
 encoder and decoder models, we can reproduce these unknown tokens for the
 output sequence by copying them directly from the input.
 This is because during copying we are referring to the position of the
 token itself in the input, thus making it invariant of whether the token
 itself exists in the vocabulary.
\end_layout

\begin_layout Standard
We therefore introduce the concept of assigning temporary indices to OOVs
 for each sample.
 For each sample, we create a temporary dictionary 
\begin_inset Formula $OOV$
\end_inset

 that maps each unknown token from an input 
\begin_inset Formula $S$
\end_inset

+Q with a temporary index that starts from 
\begin_inset Formula $|V|$
\end_inset

.
 We then reorganize the probability vectors 
\begin_inset Formula $\mathbf{p}_{t}^{gen}$
\end_inset

 and 
\begin_inset Formula $\mathbf{\mathbf{p}_{t}^{copy}}$
\end_inset

 by dividing them into tokens that belong to the original vocabulary 
\begin_inset Formula $V$
\end_inset

 and others that belong to the temporary 
\begin_inset Formula $OOV$
\end_inset

.
 The new vectors are derived as follows:
\begin_inset Formula 
\begin{equation}
\mathbf{p}_{t}^{V}\left[i\right]=\mathbf{p}_{t}^{gen}\left[i\right]+\sum\limits _{j=1}^{len(X)}\ensuremath{\text{if}\,\thinspace}x_{j}\text{==\ensuremath{V_{i}}}\thinspace\,\text{then}\thinspace\,\mathbf{\mathbf{p}_{\mathit{t}}^{copy}\left[j\right]}\thinspace\,\text{else}\,\ensuremath{\,}\ensuremath{0}\label{eq:pV}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\mathbf{p}_{t}^{OOV}\left[i\right]=\sum\limits _{j=1}^{len(X)}\ensuremath{\text{if}\,\thinspace}x_{j}\text{==\ensuremath{OOV_{i}}}\thinspace\,\text{then}\,\thinspace\mathbf{p}_{t}^{copy}\left[\mathbf{j}\right]\,\thinspace\text{else\,}\ensuremath{\,}\ensuremath{0}\label{eq:pOOV}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
y_{t}=\text{argmax\ensuremath{\left(\left[\mathbf{p}_{t}^{V},\mathbf{p}_{t}^{OOV}\right]\right)}}\label{eq:output}
\end{equation}

\end_inset


\begin_inset Formula 
\begin{equation}
\mathcal{L}_{2}=-{\displaystyle \sum_{t=1}^{len(Y)}log}\left[p\left(y_{t}|y_{<t},X\right)\right]\label{eq:loss2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $V_{i}$
\end_inset

 and 
\begin_inset Formula $OOV_{i}$
\end_inset

 each indicate the 
\begin_inset Formula $i$
\end_inset

th token of the corresponding vocabulary.
 We can thus predict the next token 
\begin_inset Formula $y_{t}$
\end_inset

 by selecting the max value of both 
\begin_inset Formula $\mathbf{p}_{t}^{V}$
\end_inset

 and 
\begin_inset Formula $\mathbf{p}_{t}^{OOV}$
\end_inset

.
\end_layout

\begin_layout Standard
Since the word embedding matrix is of size 
\begin_inset Formula $\mathbb{\mathbb{R}}^{|V|\times d_{w}}$
\end_inset

, we cannot embed and thus encode unknown words into hidden states for our
 encoders.
 However, by temporarily assigning unique indices to the unknown tokens
 instead of treating them all as 
\begin_inset Formula $\left\langle UNK\right\rangle $
\end_inset

, we can represent the output sequence using these temporary indices if
 they were selected as a result of copying.
 This convention combined with the model's capability to copy from positions
 saves us from the burden of having to restrict ourselves to a fixed and
 presumably very large vocabulary.
\end_layout

\begin_layout Subsubsection
Training
\end_layout

\begin_layout Standard
Our model is trained on two losses based on negative log-likelihood; one
 created from predicting the reference line out of 
\begin_inset Formula $N$
\end_inset

 source lines (Eq.
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss1"

\end_inset

), and another from predicting a sequence of tokens (Eq.
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset ref
LatexCommand ref
reference "eq:loss2"

\end_inset

).
 We sum these two losses to create a single loss, which we train our model
 to minimize via stochastic gradient descent..
\end_layout

\end_body
\end_document
