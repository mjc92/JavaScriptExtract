#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Subsection
Task definition
\end_layout

\begin_layout Standard
We define our proposed task as can be seen at 
\shape italic

\begin_inset CommandInset ref
LatexCommand ref
reference "fig:task"

\end_inset


\shape default
.
 Here we are given a source code snippet consisting of 
\begin_inset Formula $N+1$
\end_inset

 lines, where we denote the first 
\begin_inset Formula $N$
\end_inset

 lines as 
\series bold
source lines
\series default
 and the 
\begin_inset Formula $N+1$
\end_inset

th line as a 
\series bold
query line
\series default
.
 The query line is an incomplete line of source code which has to be completed
 by referring to previous lines of code and learning to generate the rest
 of the query line, which we denote as the 
\series bold
target line
\series default
.
 We hypothesize that there is at least one line of code from the source
 lines, namely the 
\series bold
reference line
\series default
, which contains structural information and relevant tokens that are required
 to reconstruct the target line.
 As the target line to be generated is in fact the remaining part of the
 incomplete query line, the reference line should be at least partially
 similar to the query line.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\begin_inset Graphics
	filename D:/Dropbox/progcopynet/figures/figure_task_overview/task.pdf
	width 150col%
	BoundingBox 0bp 0bp 960bp 500bp

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Model overview
\end_layout

\end_inset


\begin_inset CommandInset label
LatexCommand label
name "fig:task"

\end_inset


\end_layout

\end_inset

In this sense we can divide our task into two steps: (1) to correctly select
 the reference line out of 
\begin_inset Formula $N$
\end_inset

 source lines by selecting the line that bears most resemblance to the query
 line, and (2) to use both the selected reference line and the query line
 to learn the line structure and tokens required for predicting the target
 line.
 This leads us to designing a two-staged neural network model which is suitable
 for such tasks.
\end_layout

\begin_layout Subsection
Model description
\end_layout

\begin_layout Standard
While explaining our model, we will refer to the source lines as 
\begin_inset Formula $S$
\end_inset

 or 
\begin_inset Formula $\left[S_{1},S_{2},...,S_{n}\right]$
\end_inset

, the query line as 
\begin_inset Formula $Q$
\end_inset

, and the desired target line as 
\begin_inset Formula $T$
\end_inset

.
 Concatenating 
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 results in a full line of source code.
 Note that each line is considered as a list of source code tokens, and
 all tokens in 
\begin_inset Formula $S$
\end_inset

,
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $T$
\end_inset

 are represented as integers corresponding to the index of that token from
 a predefined vocabulary 
\begin_inset Formula $V$
\end_inset

.
 We use the notation 
\begin_inset Formula $len\left(X\right)$
\end_inset

 to indicate the length of a code line 
\begin_inset Formula $X$
\end_inset

.
 We represent the 
\begin_inset Formula $j$
\end_inset

th token of source line 
\begin_inset Formula $i$
\end_inset

 and the query as 
\begin_inset Formula $s_{i,j}$
\end_inset

 and 
\begin_inset Formula $q_{j}$
\end_inset

 respectively.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\begin_inset Graphics
	filename D:/Dropbox/progcopynet/figures/figure_model_overview/figure.pdf
	scale 50
	BoundingBox 0bp 0bp 960bp 500bp

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Model overview
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsubsection
Selector network
\end_layout

\begin_layout Standard
As previously mentioned, our first task is to select the reference line
 
\begin_inset Formula $S_{ref}$
\end_inset

 from 
\begin_inset Formula $S$
\end_inset

 by comparing the similarity of each line 
\begin_inset Formula $S_{i}$
\end_inset

 with 
\begin_inset Formula $Q$
\end_inset

.
 As each line is a list of tokens, it makes sense that we can compare two
 lines by comparing the tokens from each line.
 Although each of the lines are likely to have different lengths, our only
 concern here is the number of tokens in 
\begin_inset Formula $Q$
\end_inset

, or 
\begin_inset Formula $len\left(Q\right)$
\end_inset

.
 Therefore, the task of measuring the similarity of the 
\begin_inset Formula $i$
\end_inset

th line of 
\begin_inset Formula $S$
\end_inset

 with 
\begin_inset Formula $Q$
\end_inset

 becomes analogous to comparing 
\begin_inset Formula $s_{i,j}$
\end_inset

 and 
\begin_inset Formula $q_{j}$
\end_inset

 where 
\begin_inset Formula $1\leq j\leq len(Q)$
\end_inset

, and add up the similarity scores for each tokenwise pair.
\end_layout

\begin_layout Standard
Then how can we measure similarities between tokens? While the most straightforw
ard approach may be to directly compare the similarity of tokens as strings
 and add up the number of identical tokens as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\begin{alignedat}{1}tok\_similarity\left(S_{i},Q\right)= & \text{\ensuremath{\left(\sum\limits _{j=1}^{len(\text{Q})}\ensuremath{1\,}\ensuremath{if\,}\ensuremath{s_{i,j}\,\text{==\ensuremath{\,q_{j}\,}}else\,}\ensuremath{0}\right)}\,/\,\ensuremath{len(Q)}}\end{alignedat}
\label{eq:sim1}
\end{equation}

\end_inset

, we adopt a more sophisticated approach that incorporates information of
 both a token's meaning and surrounding semantics.
 We first create a word embedding matrix 
\begin_inset Formula $W_{emb}$
\end_inset

of size 
\begin_inset Formula $\mathbb{R}^{\left|V\right|\times d_{w}}$
\end_inset

, then embed each token of 
\begin_inset Formula $S_{i}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 to obtain word embedding vectors, where we denote as 
\begin_inset Formula $s^{emb}$
\end_inset

 and 
\begin_inset Formula $q^{emb}$
\end_inset

.
 We then apply a bidirectional LSTM on these embeddings to obtain hidden
 states 
\begin_inset Formula $s^{h}$
\end_inset

 and 
\begin_inset Formula $q^{h}$
\end_inset

.
 These hidden states are created by incorporating token information from
 both forwards and backwards, and thus contain information of the surrounding
 tokens.
 We modify Eq.
\begin_inset space \thinspace{}
\end_inset

(1) asve
\begin_inset Formula 
\begin{equation}
vec\_similarity\left(S_{i},Q\right)=\text{\ensuremath{\left(\sum\limits _{j=1}^{len(\text{Q})}cos\left(s_{i,j}^{h},q_{j}^{h}\right)\right)}}\label{eq:sim2}
\end{equation}

\end_inset

where we use vectors instead of raw tokens.To change the similarity scores
 to a differentiable probability distribution , we apply a softmax function
 on the 
\begin_inset Formula $n$
\end_inset

 similarity scores as
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.2in}
\end_layout

\end_inset


\begin_inset Formula 
\begin{equation}
p(y=i|q)=\frac{exp\left(vec\_similarity(S_{i},Q)\right)}{\Sigma_{k=1}^{n}exp\left(vec\_similarity(S_{i},Q)\right)}\label{eq:soft}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
.
 We can then choose the most similar line from 
\begin_inset Formula $S$
\end_inset

 by selecting the line with the highest probability.
\end_layout

\begin_layout Subsubsection
Copy-generator network
\end_layout

\begin_layout Standard
This model takes in as inputs the reference line selected from 
\begin_inset Formula $S$
\end_inset

 which we denote as 
\begin_inset Formula $S_{ref}$
\end_inset

 and the query line 
\begin_inset Formula $Q$
\end_inset

.
 The objective of this model is to copy tokens selectively from 
\begin_inset Formula $S_{ref}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 to generate the corresponding target sequence 
\begin_inset Formula $T$
\end_inset

.
 Our model, based on Copynet
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "gu16copynet"

\end_inset

, is capable of generating a target sequence from a source sequence by discrimin
ating from when to generate new tokens from previous hidden states and when
 to copy directly from source tokens.
\end_layout

\begin_layout Standard
We concatenate 
\begin_inset Formula $S_{ref}$
\end_inset

 and 
\begin_inset Formula $Q$
\end_inset

 to obtain the input sequence 
\begin_inset Formula $X$
\end_inset

, and obtain word vectors 
\begin_inset Formula $\left\{ x_{1},...,x_{len(X)}\right\} $
\end_inset

 using 
\begin_inset Formula $W_{emb}.$
\end_inset

 We feed this input into an encoder model which is a bidirectional GRU to
 obtain the hidden states 
\begin_inset Formula $e_{1},...,e_{len(X)}$
\end_inset

 for each input token, i.e.
 
\begin_inset Formula 
\begin{equation}
e_{i}=GRU^{enc}\left(e_{i-1},x_{i}\right),\thinspace i\in\left\{ 1,\ldots,len(X)\right\} \label{eq:enc}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
with 
\series bold

\begin_inset Formula $e_{0}$
\end_inset


\series default
 set to a zero vector.
 We then use the last hidden state 
\begin_inset Formula $e_{len(X)}$
\end_inset

 as an encoded version of the input sequence, and feed it into the subsequent
 decoder model.
 At the decoder model, another GRU is used to generate hidden states as
 
\begin_inset Formula 
\begin{equation}
h_{i}=GRU^{dec}\left(h_{i-1},\left[c_{i},t_{i-1}\right]\right)\label{eq:dec}
\end{equation}

\end_inset

 where 
\begin_inset Formula $h_{i}$
\end_inset

 denotes the 
\begin_inset Formula $i$
\end_inset

th hidden state for the decoder, 
\begin_inset Formula $c_{i}$
\end_inset

 denotes the context vector that is an abstraction of the input hidden states
 according to attention scores, and 
\begin_inset Formula $t_{i}$
\end_inset

 is the word embedding of the 
\begin_inset Formula $i$
\end_inset

th decoder input.
 Each hidden state of the decoder 
\begin_inset Formula $h_{i}$
\end_inset

 is used to predict the next word in two different ways: 1) to obtain the
 attention scores of which part of the input sequence to attend, and 2)
 to directly apply a linear transformation for generating the next word
 from a large vocabulary.
\end_layout

\begin_layout Standard
During 1), the output distribution of words is obtained as 
\begin_inset Formula 
\begin{equation}
gen_{t}=h_{t}^{T}W_{o},\thinspace W_{o}\in\mathbb{R}^{d_{h}\times|V|}\label{eq:gen}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
where 
\begin_inset Formula $d_{h}$
\end_inset

 is the hidden state size.
 The output is a 
\begin_inset Formula $|V|$
\end_inset

-dimensional vector which indicates the probability of generating a new
 word that exists in the vocabulary.
\end_layout

\begin_layout Standard
For 2), we use 
\begin_inset Formula $h_{t}$
\end_inset

 to obtain attention scores for each of the hidden states obtained from
 the input sequence as 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
att_{t,i}=tanh\left(h_{t}^{T}e_{i}\right),\thinspace i\in\left\{ 1,\ldots,len(X)\right\} \label{eq:att}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
, which is the attention score for the 
\begin_inset Formula $i$
\end_inset

th token in the input sequence.
 For each decoder step 
\begin_inset Formula $t$
\end_inset

 we can calculate an attention vector 
\begin_inset Formula $att_{t}\in\mathbb{\mathbb{R}}^{len(X)}$
\end_inset

, which indicates how relevant each of the input tokens are to the word
 to be predicted at decoder step 
\begin_inset Formula $t$
\end_inset

.
 By concatenating 
\begin_inset Formula $att_{t}$
\end_inset

 and 
\begin_inset Formula $gen_{t}$
\end_inset

 and applying softmax, we can get a probability distribution
\begin_inset Formula 
\begin{equation}
\left[prob_{t}^{copy},prob_{t}^{gen}\right]=softmax\left(\left[att_{t},gen_{t}\right]\right)\label{eq:out}
\end{equation}

\end_inset

which contains both the probabilities of either copying the next token from
 the one of the input tokens or generating it directly using the hidden
 state.
 If the largest value is in the 
\begin_inset Formula $k$
\end_inset

th index of the vector 
\begin_inset Formula $prob_{t}^{copy},$
\end_inset


\begin_inset Formula $x_{k}$
\end_inset

 becomes the next output token.
 On the other hand, if the largest value is located in the 
\begin_inset Formula $k$
\end_inset

th index of 
\begin_inset Formula $prob_{t}^{gen}$
\end_inset

, the output token becomes the 
\begin_inset Formula $k$
\end_inset

th word in the predefined vocabulary 
\begin_inset Formula $V$
\end_inset

.
 This way, we can leverage between generating tokens from our vocabulary
 and copying them from the input sequence.
\end_layout

\begin_layout Standard
For each decoder step, we need to calculate a context vector 
\begin_inset Formula $c_{t}$
\end_inset

 which represents how much information is selectively obtained from the
 input sequence.
 This model is different from other attention-based sequence-to-sequence
 models
\begin_inset space \thinspace{}
\end_inset


\begin_inset CommandInset citation
LatexCommand cite
key "bahdanau15att"

\end_inset

 in that instead of using the entire attention vector 
\begin_inset Formula $att_{t}$
\end_inset

 to create a context vector, it only attends to positions where the input
 token is equal to the decoder input.
 In other words, if the decoder input token 
\begin_inset Formula $t_{t}$
\end_inset

 has appeared in the 
\begin_inset Formula $j$
\end_inset

th position of 
\begin_inset Formula $X$
\end_inset

, 
\begin_inset Formula $c_{t}$
\end_inset

 then takes the value of 
\begin_inset Formula $e_{j}$
\end_inset

.
 This rather direct form of attention allows for copying long sequences
 of tokens from the input as mentioned in 
\begin_inset CommandInset citation
LatexCommand cite
key "gu16copynet"

\end_inset

.
\end_layout

\begin_layout Subsubsection
Dealing with out-of-vocabulary cases
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement h
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\begin_inset Graphics
	filename D:/Dropbox/progcopynet/figures/figure_oov/oov-cropped.pdf
	width 200col%
	BoundingBox 0bp 0bp 960bp 500bp

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-.1in}
\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Temporary vocabulary for handling OOVs
\end_layout

\end_inset


\end_layout

\end_inset

One advantage of our model is that it can handle words that does not exist
 in the predefined vocabulary.
 There is an unlimited amount of tokens that can appear in a source code
 text as different users may come up with different naming conventions for
 functions and variables.
 With the exception of symbols that constitute the grammar of a language
 or identifiers in frequently used packages, most tokens inveitably have
 to be treated as unknown words.
 In natural language processing tasks, these word are treated as out-of-vocabula
ry
\begin_inset space \thinspace{}
\end_inset

(OOV) tokens and are usually replaced with an 
\begin_inset Formula $\left\langle UNK\right\rangle $
\end_inset

 token.
 While we also use this convention while feeding unknown tokens into our
 encoder and decoder models, we can reproduce these unknown tokens for the
 output sequence by copying them directly from the input.
 This is because during copying we are referring to the position of the
 token itself in the input, thus making it invariant of whether the token
 itself exists in the vocabulary.
\end_layout

\begin_layout Standard
We therefore introduce the concept of assigning temporary indices to OOVs
 for each sample.
 For each sample, we create a temporary dictionary that maps each unknown
 token from an input 
\begin_inset Formula $S$
\end_inset

+Q with a temporary index that starts from 
\begin_inset Formula $|V|$
\end_inset

.
 Since the word embedding matrix is of size 
\begin_inset Formula $\mathbb{\mathbb{R}}^{|V|\times d_{w}}$
\end_inset

, we cannot embed and thus encode unknown words into hidden states for our
 encoders.
 However, by temporarily assigning unique indices to the unknown tokens
 instead of treating them all as 
\begin_inset Formula $\left\langle UNK\right\rangle $
\end_inset

, we can represent the output sequence using these temporary indices if
 they were selected as a result of copying.
 This convention combined with the model's capability to copy from positions
 saves us from the burden of having to restrict ourselves to a fixed and
 presumably very large vocabulary.
\end_layout

\end_body
\end_document
